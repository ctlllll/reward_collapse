model_name: microsoft/deberta-v3-base
learning_rate: 1e-5
scheduler: cosine
gradient_checkpointing: false
gradient_accumulation_steps: 8
per_device_train_batch_size: 1
warmup_ratio: 0.1
eval_steps: 20
save_steps: 20
max_length: 512
num_train_epochs: 80
use_scores: False
output_dir: G_log
loss: log
datasets:
  - alpaca
